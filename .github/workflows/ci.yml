name: Client CI

on:
  push:
    branches: [main]
  pull_request:

jobs:
  client-ci: 
    runs-on: ubuntu-latest
    
    steps: 
    - uses: actions/checkout@v4

    - name: PR review gate (goal + acceptance criteria + validation)
      if: ${{ github.event_name == 'pull_request' }}
      uses: actions/github-script@v7
      with:
        script: |
          const pr = context.payload.pull_request;
          if (!pr) {
            core.info('No pull_request payload found; skipping.');
            return;
          }

          const body = pr.body || '';
          const requiredMarkers = [
            '# Summary',
            '## Goal / Acceptance Criteria (required)',
            '# Issue / Tracking Link (required)',
            '# Validation (required)',
            '## Automated checks',
            '## Manual test evidence (required)',
            '# Cross-repo / Downstream impact (always include)',
          ];
          const missing = requiredMarkers.filter(m => !body.includes(m));
          if (missing.length) {
            core.setFailed(`PR description is missing required sections: ${missing.join(', ')}`);
            return;
          }

          function sectionText(md, heading) {
            const start = md.indexOf(heading);
            if (start === -1) return '';
            const rest = md.slice(start + heading.length);
            const next = rest.search(/\n#{1,2}\s+/);
            return (next === -1 ? rest : rest.slice(0, next)).trim();
          }

          function checkboxLines(mdSection) {
            return mdSection
              .split(/\r?\n/)
              .map(l => l.trim())
              .filter(l => /^- \[[ xX]\]/.test(l));
          }

          function requireCheckboxChecked(section, pattern, label) {
            const lines = section.split(/\r?\n/).map(l => l.trim());
            const line = lines.find(l => pattern.test(l));
            if (!line) return `Missing checkbox line for: ${label}`;
            if (!/^\- \[[xX]\]/.test(line)) return `Checkbox must be checked for: ${label}`;
            return null;
          }

          // Acceptance criteria must exist and be checked
          const ac = sectionText(body, '## Goal / Acceptance Criteria (required)');
          const acBoxes = checkboxLines(ac);
          const problems = [];
          if (acBoxes.length === 0) problems.push('Acceptance Criteria section must include at least one checkbox item.');
          if (acBoxes.length && acBoxes.some(l => !/^\- \[[xX]\]/.test(l))) problems.push('All Acceptance Criteria checkboxes must be checked.');

          // Tracking link must not be placeholder underscores
          const fixesLine = (body.split(/\r?\n/).find(l => l.trim().toLowerCase().startsWith('fixes:')) || '').trim();
          const fixesValue = fixesLine.replace(/^fixes:\s*/i, '').trim();
          const fixesOk =
            fixesValue.length > 0 &&
            !/_{5,}/.test(fixesValue) &&
            !/\*{3,}/.test(fixesValue) &&
            (['none', 'n/a', 'na'].includes(fixesValue.toLowerCase()) || /#\d+/.test(fixesValue) || /https?:\/\//.test(fixesValue));
          if (!fixesOk) problems.push('Fixes: must reference a real issue/ticket (e.g. Fixes #123 or URL) or be explicitly "None"/"N/A".');

          // Validation checkboxes
          const validation = sectionText(body, '# Validation (required)');
          const auto = sectionText(body, '## Automated checks');
          const manual = sectionText(body, '## Manual test evidence (required)');

          const lintReq = requireCheckboxChecked(auto, /^- \[[ xX]\]\s+Lint passes\b/, 'Lint passes');
          if (lintReq) problems.push(lintReq);
          const buildReq = requireCheckboxChecked(auto, /^- \[[ xX]\]\s+Build passes\b/, 'Build passes');
          if (buildReq) problems.push(buildReq);
          const manualReq = requireCheckboxChecked(manual, /^- \[[ xX]\]\s+Manual test entry #1\b/, 'Manual test entry #1');
          if (manualReq) problems.push(manualReq);

          // Cross-repo impact must be explicitly filled (can be "None")
          const crossRepoLine = (body.split(/\r?\n/).find(l => l.trim().startsWith('- Related repos/services impacted:')) || '').trim();
          if (!crossRepoLine || /_{5,}/.test(crossRepoLine)) {
            problems.push('Cross-repo impact must be filled (use "None" if not applicable).');
          }

          // Lightweight implementation guard: require test changes when UI code changes.
          const files = await github.paginate(github.rest.pulls.listFiles, {
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: pr.number,
            per_page: 100,
          });
          const changed = files.map(f => f.filename);
          const uiTouched = changed.some(p => p.startsWith('client/src/'));
          const testsTouched = changed.some(p => /client\/(src\/.*\.(test|spec)\.[jt]sx?$|e2e\/|__tests__\/)/.test(p));
          if (uiTouched && !testsTouched) {
            problems.push('UI code changed under client/src/ but no client tests were updated/added (unit or e2e).');
          }

          if (problems.length) {
            core.setFailed(`PR review gate failed:\n- ${problems.join('\n- ')}`);
            return;
          }
          core.info('PR review gate passed.');

    - name: Repo hygiene guard (forbidden env files)
      shell: bash
      run: |
        set -euo pipefail
        # Forbid committing real env files; examples are allowed.
        if git ls-files | grep -E '(^|/)\.env$|(^|/)\.env\.local$|(^|/)\.env\.production$|(^|/)\.env\.e2e$' -n; then
          echo "Forbidden env files detected in git index (.env/.env.local/.env.production/.env.e2e)."
          echo "Use .env.example files instead."
          exit 1
        fi
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: 'client/package-lock.json'
    
    - name: Install dependencies
      working-directory: client
      run: npm ci
    
    - name: Run lint
      working-directory: client
      run: npm run lint
    
    - name: Build
      working-directory: client
      run: npm run build
    
    - name: Run tests
      working-directory: client
      run: npm test

  api-integration:
    needs: client-ci
    uses: blecx/AI-Agent-Framework/.github/workflows/reusable-client-api-integration.yml@main
    with:
      client_runtime: node
      node_version: '20'
      client_workdir: client
      client_install_command: npm ci
      client_lint_command: ''
      client_build_command: ''
      client_test_command: npm run test:api

  # E2E tests with smart dependency resolution
  # This job implements intelligent backend dependency resolution:
  # 1. Attempts to resolve all dependencies automatically
  # 2. Only skips tests if dependencies are truly unresolvable
  # 3. Logs detailed reasons for any skipped tests
  # For details, see docs/E2E-CI-DEPENDENCY-RESOLUTION.md
  client-e2e:
    runs-on: ubuntu-latest
    needs: client-ci
    # Run on main branch or when 'run-e2e' label is present
    if: github.ref == 'refs/heads/main' || contains(github.event.pull_request.labels.*.name, 'run-e2e')
    
    steps:
      - name: Checkout client repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'client/package-lock.json'
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install client dependencies
        working-directory: client
        run: npm ci
      
      - name: Install Playwright browsers
        working-directory: client
        run: npx playwright install --with-deps chromium
      
      - name: Attempt backend dependency resolution
        id: backend_setup
        # Note: continue-on-error is INTENTIONAL for two-run strategy
        # First run: Attempt resolution and set output variable
        # Second run: Check output variable and skip if failed
        # This allows us to capture logs and display clear skip messages
        continue-on-error: true
        run: |
          echo "=== Smart Backend Dependency Resolution ==="
          echo "Starting dependency resolution process..."
          echo ""
          
          # Set backend directory in CI environment
          export BACKEND_DIR="${GITHUB_WORKSPACE}/../AI-Agent-Framework"
          export LOG_FILE="${GITHUB_WORKSPACE}/backend-setup.log"
          
          # Run the smart setup script
          if bash client/e2e/setup-backend.sh; then
            echo "backend_available=true" >> $GITHUB_OUTPUT
            echo ""
            echo "✓ Backend setup successful"
          else
            echo "backend_available=false" >> $GITHUB_OUTPUT
            echo ""
            echo "✗ Backend setup failed"
            echo "See backend-setup.log for details"
          fi
        continue-on-error: true
      
      - name: Display backend setup log
        if: always()
        run: |
          if [ -f backend-setup.log ]; then
            echo "=== Backend Setup Log ==="
            cat backend-setup.log
          fi
      
      - name: Run E2E tests
        if: steps.backend_setup.outputs.backend_available == 'true'
        working-directory: client
        run: npx playwright test
        env:
          E2E_BASE_URL: http://localhost:5173
          API_BASE_URL: http://localhost:8000
      
      - name: Skip E2E tests with reason
        if: steps.backend_setup.outputs.backend_available != 'true'
        run: |
          echo ""
          echo "═══════════════════════════════════════════════════════════"
          echo "E2E TESTS SKIPPED - DEPENDENCY RESOLUTION FAILED"
          echo "═══════════════════════════════════════════════════════════"
          echo ""
          echo "The E2E tests were skipped because backend dependencies"
          echo "could not be resolved automatically."
          echo ""
          echo "ATTEMPTED RESOLUTION METHODS:"
          echo "  1. Clone backend repository from GitHub"
          echo "  2. Start backend via Docker Compose"
          echo "  3. Create Python venv and install dependencies"
          echo "  4. Start backend via uvicorn"
          echo ""
          echo "All methods failed. See backend-setup.log artifact for details."
          echo ""
          echo "TO FIX:"
          echo "  - Ensure backend repository is accessible"
          echo "  - Verify backend has requirements.txt or docker-compose.yml"
          echo "  - Check backend health endpoint works"
          echo ""
          echo "For more information:"
          echo "  - docs/E2E-CI-DEPENDENCY-RESOLUTION.md"
          echo "  - docs/E2E-CI-SETUP.md"
          echo ""
          echo "E2E tests will run successfully once dependencies are resolved."
          echo "═══════════════════════════════════════════════════════════"
      
      - name: Upload backend setup log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-setup-log
          path: backend-setup.log
          if-no-files-found: ignore
      
      - name: Upload Playwright report
        if: always() && steps.backend_setup.outputs.backend_available == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: client/playwright-report/
          retention-days: 30
      
      - name: Upload test screenshots
        if: failure() && steps.backend_setup.outputs.backend_available == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: test-screenshots
          path: client/test-results/
          retention-days: 30
      
      - name: Upload backend logs
        if: always() && steps.backend_setup.outputs.backend_available == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: backend-logs
          path: /tmp/backend-e2e.log
          if-no-files-found: ignore
